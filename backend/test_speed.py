"""
Speed test script for Finnish Learning API
Measures timing at word entry
"""

import requests
import time
import json
from colorama import init, Fore, Style

init(autoreset=True)

BASE_URL = "http://localhost:5003"

def test_word_speed(word):
    """Test word lookup speed with detailed timing"""
    print(f"\n{Fore.CYAN}{'='*80}")
    print(f"{Fore.CYAN}Testing word: '{word}'")
    print(f"{Fore.CYAN}{'='*80}\n")
    
    # Measure total request time (includes network + processing)
    request_start = time.time()
    
    try:
        response = requests.get(
            f"{BASE_URL}/api/word/{word}",
            params={"source_lang": "fi", "target_lang": "da"}
        )
        
        request_end = time.time()
        total_request_time = (request_end - request_start) * 1000
        
        if response.status_code == 200:
            data = response.json()
            
            # Extract timing info from response
            timing = data.get('_timing', {})
            
            print(f"{Fore.GREEN}âœ… Request successful\n")
            
            print(f"{Fore.YELLOW}â±ï¸  TIMING BREAKDOWN:")
            print(f"{Fore.WHITE}{'â”€'*80}")
            print(f"{Fore.CYAN}1. Client Request Time:      {Fore.WHITE}{total_request_time:>8.2f} ms {Fore.BLUE}(Network + Server)")
            
            if timing:
                print(f"\n{Fore.MAGENTA}Server-side breakdown:")
                print(f"{Fore.WHITE}{'â”€'*80}")
                print(f"{Fore.CYAN}   - Prompt Building:        {Fore.WHITE}{timing.get('prompt_build_ms', 0):>8.2f} ms")
                print(f"{Fore.CYAN}   - OpenAI API Call:        {Fore.WHITE}{timing.get('openai_api_ms', 0):>8.2f} ms {Fore.RED}â† Main bottleneck")
                print(f"{Fore.CYAN}   - JSON Parsing:           {Fore.WHITE}{timing.get('parse_ms', 0):>8.2f} ms")
                print(f"{Fore.WHITE}{'â”€'*80}")
                print(f"{Fore.CYAN}   Total Server Time:        {Fore.WHITE}{timing.get('total_ms', 0):>8.2f} ms")
                
                network_overhead = total_request_time - timing.get('total_ms', 0)
                print(f"{Fore.CYAN}   Network Overhead:         {Fore.WHITE}{network_overhead:>8.2f} ms")
            
            print(f"\n{Fore.GREEN}{'â”€'*80}")
            print(f"{Fore.GREEN}TOTAL TIME (End-to-End):     {Fore.WHITE}{total_request_time:>8.2f} ms")
            print(f"{Fore.GREEN}{'â”€'*80}")
            
            # Show word data
            print(f"\n{Fore.YELLOW}ðŸ“š RESULT:")
            print(f"{Fore.WHITE}Word:        {data.get('word')}")
            print(f"{Fore.WHITE}Translation: {data.get('translation')}")
            print(f"{Fore.WHITE}Pronunciation: {data.get('pronunciation')}")
            
            # Performance assessment
            print(f"\n{Fore.YELLOW}ðŸ“Š ASSESSMENT:")
            if total_request_time < 1000:
                print(f"{Fore.GREEN}âš¡ Excellent - Very fast response!")
            elif total_request_time < 2000:
                print(f"{Fore.CYAN}ðŸ‘ Good - Acceptable response time")
            elif total_request_time < 3000:
                print(f"{Fore.YELLOW}âš ï¸  Moderate - Slightly slow")
            else:
                print(f"{Fore.RED}ðŸŒ Slow - Consider optimization")
            
            print(f"\n{Fore.BLUE}ðŸ’¡ NOTE: OpenAI API call typically takes 1-3 seconds")
            print(f"{Fore.BLUE}   This is the expected behavior for AI-generated content.")
            
        else:
            print(f"{Fore.RED}âŒ Error: {response.status_code}")
            print(response.text)
            
    except Exception as e:
        print(f"{Fore.RED}âŒ ERROR: {str(e)}")

def test_multiple_words():
    """Test multiple words and show average timing"""
    print(f"\n{Fore.CYAN}{Style.BRIGHT}{'='*80}")
    print(f"MULTIPLE WORD SPEED TEST")
    print(f"{'='*80}\n")
    
    words = ["talo", "kissa", "kirja", "vesi", "auto"]
    times = []
    
    for word in words:
        start = time.time()
        try:
            response = requests.get(f"{BASE_URL}/api/word/{word}")
            end = time.time()
            elapsed = (end - start) * 1000
            times.append(elapsed)
            
            if response.status_code == 200:
                data = response.json()
                server_time = data.get('_timing', {}).get('total_ms', 0)
                print(f"{Fore.GREEN}âœ… {word:10} - {elapsed:7.2f} ms (Server: {server_time:7.2f} ms)")
            else:
                print(f"{Fore.RED}âŒ {word:10} - Failed")
        except Exception as e:
            print(f"{Fore.RED}âŒ {word:10} - Error: {str(e)}")
    
    if times:
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        
        print(f"\n{Fore.YELLOW}{'â”€'*80}")
        print(f"{Fore.CYAN}STATISTICS:")
        print(f"{Fore.WHITE}Average: {avg_time:7.2f} ms")
        print(f"{Fore.WHITE}Fastest: {min_time:7.2f} ms")
        print(f"{Fore.WHITE}Slowest: {max_time:7.2f} ms")

def test_cache_performance():
    """Test cache performance - should be milliseconds!"""
    print(f"\n{Fore.CYAN}{Style.BRIGHT}{'='*80}")
    print(f"CACHE PERFORMANCE TEST")
    print(f"{'='*80}\n")
    
    word = "talo"
    
    # First call - cold (no cache)
    print(f"{Fore.YELLOW}1ï¸âƒ£  First call (COLD - no cache):")
    start = time.time()
    response1 = requests.get(f"{BASE_URL}/api/word/{word}")
    time1 = (time.time() - start) * 1000
    
    if response1.status_code == 200:
        data1 = response1.json()
        timing1 = data1.get('_timing', {})
        print(f"{Fore.RED}   â±ï¸  Time: {time1:.2f} ms (OpenAI: {timing1.get('openai_api_ms', 0):.2f} ms)")
        print(f"{Fore.RED}   ðŸŒ SLOW - OpenAI API call required")
    
    # Second call - hot (cached)
    print(f"\n{Fore.YELLOW}2ï¸âƒ£  Second call (HOT - cached):")
    start = time.time()
    response2 = requests.get(f"{BASE_URL}/api/word/{word}")
    time2 = (time.time() - start) * 1000
    
    if response2.status_code == 200:
        data2 = response2.json()
        timing2 = data2.get('_timing', {})
        is_cached = timing2.get('cached', False)
        print(f"{Fore.GREEN}   â±ï¸  Time: {time2:.2f} ms (Cache: {timing2.get('cache_lookup_ms', 0):.2f} ms)")
        if is_cached:
            print(f"{Fore.GREEN}   âš¡ FAST - From cache!")
        else:
            print(f"{Fore.RED}   âŒ ERROR - Should be cached!")
    
    # Performance comparison
    speedup = time1 / time2 if time2 > 0 else 0
    print(f"\n{Fore.CYAN}{'â”€'*80}")
    print(f"{Fore.YELLOW}ðŸ“Š COMPARISON:")
    print(f"{Fore.WHITE}   Cold (no cache):  {time1:>8.2f} ms")
    print(f"{Fore.WHITE}   Hot (cached):     {time2:>8.2f} ms")
    print(f"{Fore.GREEN}   Speedup:          {speedup:>8.1f}x faster! ðŸš€")
    print(f"{Fore.CYAN}{'â”€'*80}")
    
    if time2 < 50:
        print(f"{Fore.GREEN}âœ… EXCELLENT: Cache response in milliseconds!")
    elif time2 < 100:
        print(f"{Fore.CYAN}ðŸ‘ GOOD: Cache response under 100ms")
    else:
        print(f"{Fore.YELLOW}âš ï¸  WARNING: Cache response slower than expected")

def main():
    print(f"{Fore.CYAN}{Style.BRIGHT}")
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘          Finnish Learning API - Speed Test                     â•‘")
    print("â•‘          Testing: http://localhost:5003                        â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    # Test cache performance first
    test_cache_performance()
    
    # Test single word with detailed timing
    test_word_speed("kissa")
    
    # Test multiple words
    test_multiple_words()
    
    print(f"\n{Fore.GREEN}âœ… Speed tests complete!\n")
    print(f"{Fore.CYAN}ðŸ’¡ TIP: First call to any word is slow (OpenAI API)")
    print(f"{Fore.CYAN}ðŸ’¡ TIP: Subsequent calls are cached and FAST (< 50ms)!\n")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print(f"\n{Fore.YELLOW}Tests interrupted by user.")
